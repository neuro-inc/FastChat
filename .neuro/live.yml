kind: live
title: fastchat

# other files from https://github.com/lm-sys/FastChat

defaults:
  life_span: 5d

images:
  fastchat:
    ref: image:$[[ project.id ]]:v1
    dockerfile: $[[ flow.workspace ]]/docker/Dockerfile
    context: $[[ flow.workspace ]]/

volumes:
  cache:
    remote: storage:$[[ flow.project_id ]]/cache
    mount: /root/.cache/huggingface
    local: cache
  project:
    remote: storage:$[[ flow.project_id ]]
    mount: /project
    local: .
  tests:
    remote: storage:$[[ flow.project_id ]]/tests
    mount: /project/tests
    local: tests

jobs:
  controller:
    image: ${{ images.fastchat.ref }}
    name: controller
    preset: cpu-mini
    http_port: "21001"
    detach: true
    bash: |
      python3.9 -m fastchat.serve.controller --host 0.0.0.0 --port 21001

  worker:
    image: ${{ images.fastchat.ref }}
    volumes:
      - ${{ volumes.cache.ref_rw }}
    env:
      FASTCHAT_CONTROLLER_URL: http://${{ inspect_job('controller').internal_hostname_named }}:21001
    preset: ${{ params.preset }}
    detach: true
    multi: true
    params:
      # https://github.com/lm-sys/FastChat/blob/main/docs/model_support.md#supported-models
      # lmsys/vicuna-7b-v1.5, lmsys/vicuna-7b-v1.5-16k,  lmsys/vicuna-13b-v1.5, lmsys/vicuna-13b-v1.5-16k, lmsys/vicuna-33b-v1.3
      # and many more
      model: lmsys/vicuna-7b-v1.5
      # model:
      preset: gpu-small
      extra: "" # e.g. --load-8bit
    bash: |
      python3.9 -m fastchat.serve.model_worker \
        --model-name ${{ params.model }} \
        --model-path ${{ params.model }} \
        --worker-address http://$NEURO_JOB_INTERNAL_HOSTNAME_NAMED:21002 \
        --controller-address $FASTCHAT_CONTROLLER_URL \
        --host 0.0.0.0 --port 21002 \
        --num-gpus `nvidia-smi --query-gpu=name --format=csv,noheader | wc -l` ${{ params.extra }}

  api:
    image: ${{ images.fastchat.ref }}
    env:
      FASTCHAT_CONTROLLER_URL: http://${{ inspect_job('controller').internal_hostname_named }}:21001
    http_port: "8000"
    name: api
    preset: cpu-mini
    detach: true
    bash: |
      python3.9 -m fastchat.serve.openai_api_server \
        --controller-address $FASTCHAT_CONTROLLER_URL \
        --host 0.0.0.0 --port 8000

  web:
    image: ${{ images.fastchat.ref }}
    env:
      FASTCHAT_CONTROLLER_URL: http://${{ inspect_job('controller').internal_hostname_named }}:21001
    http_port: "8000"
    preset: cpu-mini
    name: web
    detach: true
    browse: true
    bash: |
      python3.9 -m fastchat.serve.gradio_web_server \
        --controller-url $FASTCHAT_CONTROLLER_URL \
        --model-list-mode reload \
        --host 0.0.0.0 --port 8000

  locust:
    image: locustio/locust
    http_port: 8089
    preset: cpu-small
    detach: True
    name: locust
    env:
      NEURO_TOKEN: secret:NEURO_TOKEN
    volumes:
      - ${{ volumes.tests.ref_ro }}
    cmd: -f ${{ volumes.tests.mount }}/locustfile.py -H http://${{ inspect_job('api').internal_hostname_named }}:8000
